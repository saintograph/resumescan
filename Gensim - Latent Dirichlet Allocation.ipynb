{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import smart_open\n",
    "\n",
    "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "    with smart_open.open(url, \"rb\") as file:\n",
    "        with tarfile.open(fileobj=file) as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', member.name):\n",
    "                    member_bytes = tar.extractfile(member).read()\n",
    "                    yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "docs = list(extract_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "387 \n",
      "Neural Net and Traditional Classifiers  \n",
      "William Y. Huang and Richard P. Lippmann \n",
      "MIT Lincoln Laboratory \n",
      "Lexington, MA 02173, USA \n",
      "Abstract\n",
      "Previous work on nets with continuous-valued inputs led to generative \n",
      "procedures to construct convex decision regions with two-layer percepttons (one hidden \n",
      "layer) and arbitrary decision regions with three-layer percepttons (two hidden layers). \n",
      "Here we demonstrate that two-layer perceptton classifiers trained with back propagation \n",
      "can form both c\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(docs[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387 \n",
      "Neural Net and Traditional Classifiers  \n",
      "William Y. Huang and Richard P. Lippmann \n",
      "MIT Lincoln Laboratory \n",
      "Lexington, MA 02173, USA \n",
      "Abstract\n",
      "Previous work on nets with continuous-valued inputs led to generative \n",
      "procedures to construct convex decision regions with two-layer percepttons (one hidden \n",
      "layer) and arbitrary decision regions with three-layer percepttons (two hidden layers). \n",
      "Here we demonstrate that two-layer perceptton classifiers trained with back propagation \n",
      "can form both convex and disjoint decision regions. Such classifiers are robust, train \n",
      "rapidly, and provide good performance with simple decision regions. When complex \n",
      "decision regions are required, however, convergence time can be excessively long and \n",
      "performance is often no better than that of k-nearest neighbor classifiers. Three neural \n",
      "net classifiers are presented that provide more rapid training under such situations. \n",
      "Two use fixed weights in the first one or two layers and are similar to classifiers that \n",
      "estimate probability density functions using histograms. A third \"feature map classifier\" \n",
      "uses both unsupervised and supervised training. It provides good performance with \n",
      "little supervised training in situations such as speech recognition where much unlabeled \n",
      "training data is available. The architecture of this classifier can be used to implement \n",
      "a neural net k-nearest neighbor classifier. \n",
      "1. INTRODUCTION \n",
      "Neural net architectures can be used to construct many different types of classi- \n",
      "tiers [7]. In particular, multi-layer perceptron classifiers with continuous valued in- \n",
      "puts trained with back propagation are robust, often train rapidly, and provide perfor- \n",
      "mance similar to that provided by Gaussian classifiers when decision regions are convex \n",
      "[12,7,5,8]. Generative procedures demonstrate that such classifiers can form convex deci- \n",
      "sion regions with two-layer perceptrons (one hidden layer) and arbitrary decision regions \n",
      "with three-layer perceptrons (two hidden layers) [7,2,9]. More recent work has demon- \n",
      "strated that two-layer perceptrons can form non-convex and disjoint decision regions. \n",
      "Examples of hand crafted two-layer networks which generate such decision regions are \n",
      "presented in this paper along with Monte Carlo simulations where complex decision \n",
      "regions were generated using back propagation training. These and previous simula- \n",
      "tions [5,8] demonstrate that convergence time with back propagation can be excessive \n",
      "when complex decision regions are desired and performance is often no better than that \n",
      "obtained with k-nearest neighbor classifiers [4]. These results led us to explore other \n",
      "neural net classifiers that might provide faster convergence. Three classifiers called, \n",
      "\"fixed weight,\" \"hypercube,\" and \"feature map\" classifiers, were developed and eval- \n",
      "uated. All classifiers were tested on illustrative problems with two continuous-valued \n",
      "inputs and two classes (A and B). A more restricted set of classifiers was tested with \n",
      "vowel formant data. \n",
      "2. CAPABILITIES OF TWO LAYER PERCEPTRONS \n",
      "Multi-layer perceptron classifiers with hard-limiting nonlinearities (node outputs \n",
      "of 0 or 1) and continuous-valued inputs can form complex decision regions. Simple \n",
      "constructive proofs demonstrate that a three-layer perceptron (two hidden layers) can \n",
      " This work was sponsored by the Defense Advanced Research Projects Agency and the Department \n",
      "of the Air Force. The views expressed are those of the authors and do not reflect the policy or position \n",
      "of the U.S. Government. \n",
      "American Institute of Physics 1988 \n",
      "388 \n",
      "b I b2 \n",
      "Xl x 2 \n",
      "DECISION REGION FOR CLASS A \n",
      "x I bl b2 \n",
      " b6 ........ \n",
      "-2 \n",
      "I I \n",
      "I \n",
      " I \n",
      "I \n",
      "o I 2 \n",
      "3 4 \n",
      "Xl \n",
      "FIG. 1. A two-layer perceptton that forms disjoint decision re9ions for class A (shaded areas). Connec- \n",
      "tion weights and node offsets are shown in the left. Hyperplanes formed by all hidden nodes are drawn \n",
      "as dashed lines with node labels. Arrows on these lines point to the half plane where the hidden node \n",
      "output is \"high\" \n",
      "form arbitrary decision regions and a two-layer perceptron (one hidden layer) can form \n",
      "single convex decision regions [7,2,9]. Recently, however, it has been demonstrated that \n",
      "two-layer perceptrons can form decision regions that are not simply convex [14]. Fig. 1, \n",
      "for example, shows how disjoint decision regions can be generated using a two-layer \n",
      "perceptron. The two disjoint shaded areas in this Fig. represent the decision region \n",
      "for class A (output node has a \"high\" output, y = 1). The remaining area represents \n",
      "the decision region for class B (output node has a \"low\" output, y = 0). Nodes in \n",
      "this Fig. contain hard-limiting nonlinearities. Connection weights and node offsets are \n",
      "indicated in the left diagram. Ten other complex decision regions formed using two-layer \n",
      "perceptrons are presented in Fig. 2. \n",
      "The above examples suggest that two-layer perceptrons can form decision regions \n",
      "with arbitrary shapes. We, however, know of no general proof of this capability. A \n",
      "1965 book by Nilson discusses this issue and contains a proof that two-layer nets can \n",
      "divide a finite number of points into two arbitrary sets ([10] page 89). This proof \n",
      "involves separating M points using at most M - 1 parallel hyperplanes formed by first- \n",
      "layer nodes where no hyperplane intersects two or more points. Proving that a given \n",
      "decision region can be formed in a two-layer net involves testing to determine whether \n",
      "the Boolean representations at the output of the first layer for all points within the \n",
      "decision region for class A are linearly separable from the Boolean representations for \n",
      "class B. One test for linear separability was presented in 1962 [13]. \n",
      "A problem with forming complex decision regions with two-layer percepttons is that \n",
      "weights and offsets must be adjusted carefully because they interact extensively to form \n",
      "decision regions. Fig. 1 illustrates this sensitivity problem. Here it can be seen that \n",
      "weights to one hidden node form a hyperplane which influences decision regions in \n",
      "an entire halLplane. For example, small errors in first layer weights that results in a \n",
      "change in the slopes of hyperplanes b$ and b6 might only slightly extend the A region \n",
      "but completely eliminate the A2 region. This interdependence can be eliminated in \n",
      "three layer perceptrons. \n",
      "It is possible to train two-layer percepttons to form complex decision regions using \n",
      "back propagation and sigmoidal nonlinearities despite weight interactions. Fig. 3, for \n",
      "example, shows disjoint decision regions formed using back propagation for the problem \n",
      "of Fig. 1. In this and all other simulations, inputs were presented alternately from \n",
      "classes A and B and selected from a uniform distribution covering the desired decision \n",
      "region. In addition, the back propagation rate of descent term, r/, was set equal to the \n",
      "momentum gain term, a and r/= a = .01. Small values for r/and a were necessary to \n",
      "guarantee convergence for the difficult problems in Fig. 2. Other simulation details are \n",
      "389 \n",
      "I) ) s) ) I \n",
      "FIG. 2. Ten complex decision regions formed by two-layer perceptrons. The numbers assigned to each \n",
      "case are the acase\"numbers used in the rest of this paper. \n",
      "as in [5,8]. Also shown in Fig. 3 are hyperplanes formed by those first-layer nodes with \n",
      "the strongest connection weights to the output node. These hyperplanes and weights \n",
      "are similar to those in the networks created by hand except for sign inversions, the \n",
      "occurrence of multiple similar hyperplanes formed by two nodes, and the use of node \n",
      "offsets with values near zero. \n",
      "3. COMPARATIVE RESULTS OF TwO-LAYERS VS. THREE-LAYERS \n",
      "Previous results [5,8], as well as the weight interactions mentioned above, suggest \n",
      "that three-layer percepttons may be able to form complex decision regions faster with \n",
      "back propagation than two-layer percepttons. This was explored using Monte Carlo \n",
      "simulations for the first nine cases of Fig. 2. All networks have 32 nodes in the first \n",
      "hidden layer. The number of nodes in the second hidden layer was twice the number \n",
      "of convex regions needed to form the decision region (2, 4, 6, 4, 6, 6, 8, 6 and 6 for \n",
      "Cases I through 9 respectively). Ten runs were typically averaged together to obtain \n",
      "a smooth curve of percentage error vs. time (number of training trials) and enough \n",
      "trials were run (to a limit of 250,000) until the curve appeared to fiatten out with little \n",
      "improvement over time. The error curve was then low-pass filtered to determine the \n",
      "convergence time. Convergence time was defined as the time when the curve crossed a \n",
      "value 5 percentage points above the final percentage error. This definition provides a \n",
      "framework for comparing the convergence time of the different classifiers. It, however, is \n",
      "not the time after which error rates do not improve. Fig. 4 summarizes results in terms \n",
      "of convergence time and final percentage error. In those cases with disjoint decision \n",
      "regions, back propagation sometimes failed to form separate regions after 250,000 trials. \n",
      "For example, the two disjoint regions required in Case 2 were never fully separated with \n",
      "390 \n",
      "FIG. 3. Decision regions formed using back propagation for Cases  of Fig. . Thick solid lines represent \n",
      "decision boundaries. Dashed lines and arrows have the same meaning as in Fig. 1. Only hyperplanes \n",
      "for hidden nodes with large weights to the output node are shown. Over 300,000 training trials were \n",
      "required to form separate regions. \n",
      "a two-layer perceptron but were separated with a three-layer perceptron. This is noted \n",
      "by the use of filled symbols in Fig. 4. \n",
      "Fig. 4 shows that there is no significant performance difference between two and \n",
      "three layer perceptrons when forming complex decision regions using back propagation \n",
      "training. Both types of classifiers take an excessively long time (> 100,000 trials) to \n",
      "form complex decision regions. A minor difference is that in Cases 2 and 7 the two-layer \n",
      "network failed to separate disjoint regions after 250,000 trials whereas the three-layer \n",
      "network was able to do so. This, however, is not significant in terms of convergence time \n",
      "and error rate. Problems that are difficult for the two-layer networks are also difficult \n",
      "for the three-layer networks, and vice versa. \n",
      "4. ALTERNATIVE CLASSIFIERS \n",
      "Results presented above and previous results [5,8] demonstrate that multi-layer per- \n",
      "ceptron classifiers can take very long to converge for complex decision regions. Three \n",
      "alternative classifiers were studied to determine whether other types of neural net clas- \n",
      "sifiers could provide faster convergence. \n",
      "4.1. FIXED WEIGHT CLASSIFIERS \n",
      "Fixed weight classifiers attempt to reduce training time by adapting only weights \n",
      "between upper layers of multi-layer perceptrons. Weights to the first layer are fixed \n",
      "before training and remain unchanged. These weights form fixed hyperplanes which \n",
      "can be used by upper layers to form decision regions. Performance will be good if the \n",
      "fixed hyperplanes are near the decision region boundaries that are required in a specific \n",
      "problem. Weights between upper layers are trained using back propagation as described \n",
      "above. Two methods were used to adjust weights to the first layer. Weights were \n",
      "adjusted to place hyperplanes randomly or in a grid in the region (-1 < x,x2 < 10). \n",
      "All decision regions in Fig. 2 fall within this region. Hyperplanes formed by first layer \n",
      "nodes for \"fixed random\" and \"fixed grid\" classifiers for Case 2 of Fig. 2 are shown as \n",
      "dashed lines in Fig. 5. Also shown in this Fig. are decision regions (shaded areas) formed \n",
      "391 \n",
      "% \n",
      "12 \n",
      "10 \n",
      "8 \n",
      "6 \n",
      "4 \n",
      "2 \n",
      "O \n",
      "I I I I i i I i \n",
      "[2] 2- layers ERROR KATE \n",
      "O 3- layers \n",
      "I I I J I I I J \n",
      "200000 I I I I I / I I I \n",
      "CONVERGENCE TIME \n",
      "1OOOOO \n",
      "50000 \n",
      "0 I I I I I I I \n",
      "1 2 3 4 5 6 7 8 9 \n",
      "Case Numbers (see Fig. 2) \n",
      "Fro. 4. Perce.taae error (top) a.d co-oer9e.ce time (bottom) for Cases I throu9h 9 of Fi9.  for \n",
      "ttvo-and three.layer perceptton classifiers trained usin 9 back propa9ation, Filled syrabois indicate that \n",
      "separate disjoint re9ions tvere not formed after 50,000 trials. \n",
      "using back propagation to train only the upper network layers. These regions illustrate \n",
      "how fixed hyperplanes are combined to form decision regions. It can be seen that decision \n",
      "boundaries form along the available hyperplanes. A good solution is possible for the \n",
      "fixed grid classifier where desired decision region boundaries are near hyperplanes. The \n",
      "random grid classifier provides a poor solution because hyperplanes are not near desired \n",
      "decision boundaries. The performance of a fixed weight classifier depends both on the \n",
      "placement of hyperplanes and on the number of hyperplanes provided. \n",
      "4.2. HYPERCUBE CLASSIFIER \n",
      "Many traditional cidsifters estimate probability density functions of input variables \n",
      "for different classes using histogram techniques [4]. Hypercube cidsifters use this tech- \n",
      "nique by fixing weights in the first two layers to break the input space into hypercubes \n",
      "(squares in the case of two inputs). Hypercube classifiers are similar to fixed weight \n",
      "classifiers, except weights to the first two layers are fixed, and only weights to output \n",
      "nodes are trained. Hypercube classifiers are also similar in structure to the CMAC \n",
      "model described by Albus [1]. The output of a second layer node is \"high\" only if the \n",
      "input is in the hypercube corresponding to that node. This is illustrated in Fig. 6 for a \n",
      "network with two inputs. \n",
      "The top layer of a hypercube classifier can be trained using back propagation. A \n",
      "maximum likelihood approach, however, suggests a simpler training algorithm which \n",
      "consists of counting. The output of second layer node Hi is connected to the output \n",
      "node corresponding to that class with greatest frequency of occurrence of training inputs \n",
      "in hypercube Hi. That is, if a sample falls in hypercube Hi, then it is classified as class \n",
      "O* where \n",
      "Ni,o. > Ni,o for all 0  0'. (1) \n",
      "In this equation, Ni,o is the number of training tokens in hypercube Hi which belong to \n",
      "class O. This will be called maximum likelihood (ML) training. It can be implemented \n",
      "by connection second-layer node Hi only to that output node corresponding to class O* \n",
      "in Eq. (1). In all simulations hypercubes covered the area (-1 < x, x2 < 10). \n",
      "392 \n",
      "RANDOM \n",
      "k r_, ,..W - 4',,;-,- \n",
      "/; /l. C'( /it %. \n",
      "J ; /  ', b t :' \n",
      ",,,.,., ,,? . \n",
      "oP,',\"' '\"  \n",
      "0 I 2 3 \n",
      "GRID \n",
      "0 1 2 3 4 \n",
      "Fla. 5. Decision regions formed with 'xed random\" and 'xed grid\" classifiers for Case $ from Fig. \n",
      "$ using back propagation training. Lines shown are hIperplanes formed by the first layer nodes. Shaded \n",
      "areas represent the decision region for class A. \n",
      "A \n",
      "I B -1 H2' 1.6 H \n",
      "TRAINED \n",
      "LAYER \n",
      "FIXED \n",
      "LAYERS \n",
      "FOUR BINS CREATED \n",
      "BY FIXED LAYERS \n",
      "x 2 \n",
      "3 \n",
      "2 \n",
      "1 \n",
      "b3 \n",
      " jb6 \n",
      "m j bE \n",
      "2 3 \n",
      "xl x2 \n",
      "INPUT \n",
      "FIG. 6. A hlpercube classifier (left) is a three-lager perceptton with fixed weights to the first two layers, \n",
      "and trainable weights to output nodes. Weights are initialized such that outputs of nodes Hi through H4 \n",
      "(left) are \"high\" onil when the input is in the corresponding hIpercube (right). \n",
      "393 \n",
      "SELECT \n",
      "CLASS \n",
      "WITH MAJORITY \n",
      "IN TOP k \n",
      "SELECT TOP \n",
      "k EXEMPLARS \n",
      "CALCULATE \n",
      "CORRELATION \n",
      "TO STORED \n",
      "EXEMPLARS \n",
      "OUTPUT (Only One High) \n",
      "Yl \n",
      "INPUT \n",
      "SUPERVISED \n",
      "ASSOCIATIVE \n",
      "LEARNING \n",
      "UNSUPERVISED \n",
      "KOHONEN \n",
      "FEATURE MAP \n",
      "LEARNING \n",
      "x N \n",
      "FIG. 7. Feature map classifier. \n",
      "4.3. FEATURE MAP CLASSIFIER \n",
      "In many speech and image classification problems a large quantity of unlabeled \n",
      "training data can be obtained, but little labeled data is available. In such situations \n",
      "unsupervised training with unlabeled training data can substantially reduce the amount \n",
      "of supervised training required [3]. The feature map classifier shown in Fig. 7 uses com- \n",
      "bined supervised/unsupervised training, and is designed for such problems. It is similar \n",
      "to histogram classifiers used in discrete observation hidden Markov models [11] and the \n",
      "classifier used in [6]. The first layer of this classifier forms a feature map using a self \n",
      "organizing clustering algorithm described by Kohonen [6]. In all simulations reported in \n",
      "this paper 10,000 trials of unsupervised training were used. After unsupervised train- \n",
      "ing, first-layer feature nodes sample the input space with node density proportional to \n",
      "the combined probability density of all classes. First layer feature map nodes perform a \n",
      "function similar to that of second layer hypercube nodes except each node has maximum \n",
      "output for input regions that are more general than hypercubes and only the output of \n",
      "the node with a maximum output is fed to the output nodes. Weights to output nodes \n",
      "are trained with supervision after the first layer has been trained. Back propag.tion, or \n",
      "maximum likelihood training can be used. Maximum likelihood training reqmres Ni,e \n",
      "(Eq. 1) to be the number of times first layer node i has maximum output for inputs \n",
      "from class 0. In addition, during classification, the outputs of nodes with Ni,e = 0 for \n",
      "all 0 (untrained nodes) are not considered when the first-layer node with the maximum \n",
      "output is selected. The network architecture of a feature map classifier can be used \n",
      "to implement a k-nearest neighbor classifier. In this case, the feedback connections in \n",
      "Fig. 7 (large circular summing nodes and triangular integrators) used to select those \n",
      "k nodes with the maximum outputs must be slightly modified. K is I for a feature \n",
      "map classifier and must be adjusted to the desired value of k for a k-nearest neighbor \n",
      "classifier. \n",
      "5. COMPARISON BETWEEN CLASSIFIERS \n",
      "The results of Monte Carlo simulations using all classifiers for Case 2 are shown in \n",
      "Fig. 8. Error rates and convergence times were determined as in Section 3. All alter- \n",
      "394 \n",
      "12 \n",
      "0 \n",
      "Trials \n",
      "25OO \n",
      "2O00 \n",
      "1500 \n",
      "1000 \n",
      "5OO \n",
      "0 \n",
      "Conventional \n",
      "I \n",
      "GAUSS \n",
      "KNN \n",
      "Back-Prop \n",
      "3- lay \n",
      "Percent Correct \n",
      "Flxed Welght \n",
      "I \n",
      "I \n",
      "Hypercube \n",
      "! ! \n",
      "Feature Map \n",
      "I I \n",
      "! I \n",
      "I \n",
      "2-lay \n",
      "i I I \n",
      "KNN GAUSS 32 \n",
      "Convergence Time \n",
      "' ndom \n",
      "64K \n",
      "3-lay ra \n",
      "grid \n",
      "I I I I \n",
      "36 40 120 440 1680 \n",
      "Number of Hidden Nodes \n",
      "I I \n",
      "I I I \n",
      "B/4.51 \n",
      "100 1600 \n",
      "FIG. 8. Comparative performance of classifiers for Case �. Trainin 9 time of the feature map classifiers \n",
      "does not include the 10,000 unsupervised trainin9 trials. \n",
      "native classifiers had shorter convergence times than multi-layer perceptron classifiers \n",
      "trained with back propagation. The feature map classifier provided best performance. \n",
      "With 1,600 nodes, its error rate was similar to that of the k-nearest neighbor classifiers \n",
      "but it required fewer than 100 supervised training tokens. The larger fixed weight and \n",
      "hypercube classifiers performed well but required more supervised training than the \n",
      "feature map classifiers. These classifiers will work well when the combined probability \n",
      "density function of all classes varies smoothly and the domain where this function is \n",
      "non-zero is known. In this case weights and offsets can be set such that hyperplanes and \n",
      "hypercubes cover the domain and provide good performance. The feature map classifier \n",
      "automatically covers the domain. Fixed weight \"random\" classifiers performed substan- \n",
      "tially worse than fixed weight \"grid\" classifiers. Back propagation training (BP) was \n",
      "generally much slower than maximum likelihood training (ML). \n",
      "6. VOWEL CLASSIFICATION \n",
      "Multi layer perceptron, feature map, and traditional classifiers were tested with \n",
      "vowel formant data from Peterson and Barney [11]. These data had been obtained \n",
      "by spectrographic analysis of vowels in /hVd/ context spoken by 67 men, women and \n",
      "children. First and second formant data of ten vowels was split into two sets, resulting \n",
      "in a total of 338 training tokens and 333 testing tokens. Fig. 9 shows the test data \n",
      "and the decision regions formed by a two-layer perceptton classifier trained with back \n",
      "propagation. The performance of classifiers is presented in Table I. All classifiers had \n",
      "similar error rates. The feature map classifier with only 100 nodes required less than 50 \n",
      "supervised training tokens (5 samples per vowel class) for convergence. The perceptton \n",
      "classifier trained with back propagation required more than 50,000 training tokens. The \n",
      "first stage of the feature map classifier and the multi-layer perceptton classifier were \n",
      "trained by randomly selecting entries from the 338 training tokens after labels had been \n",
      "removed and using tokens repetitively. \n",
      "395 \n",
      "4OOO \n",
      "2000 \n",
      "F () \n",
      "lOOC \n",
      "5oo \n",
      "o head \n",
      "� hid \n",
      "� hod \n",
      " had \n",
      "o hawed \n",
      "� heard \n",
      "o heed \n",
      " hud \n",
      ") who'd \n",
      "^ hood \n",
      "FIG. 9. Decision regions formed by a two-layer network usin9 BP after �00,000 trainin9 tokens from \n",
      "Petersoh's steady state vowel data [Peterson, 195�]. Also shown are samples of the testin9 set. Le9end \n",
      "show example of the pronunciation of the 10 vowels and the error within each vowel. \n",
      "ALGORITHM TRAINING TOKENS-'[ % ERROR \n",
      "KNN :338 18.0 \n",
      "Gaussian \" 338 \" 20.4 \n",
      "2-L.ayer Pe..r'eptron ,,, 50,000 19.8 \n",
      "Feature. Map < 50 ..... 22.8 \n",
      "TABLE I \n",
      "Performance of classifiers on steady state vowel data. \n",
      "396 \n",
      "7. CONCLUSIONS \n",
      "Neural net architectures form a flexible framework that can be used to construct \n",
      "many different types of classifiers. These include Gaussian, k-nearest neighbor, and \n",
      "multi-layer perceptton classifiers as well as classifiers such as the feature map classifier \n",
      "which use unsupervised training. Here we first demonstrated that two-layer percepttons \n",
      "(one hidden layer) can form non-convex and disjoint decision regions. Back propagation \n",
      "training, however, can be extremely slow when forming complex decision regions with \n",
      "multi-layer perceptrons. Alternative classifiers were thus developed and tested. All \n",
      "provided faster training and many provided improved performance. Two were similar to \n",
      "traditional classifiers. One (hypercube classifier) can be used to implement a histogram \n",
      "classifier, and another (feature map classifier) can be used to implement a k-nearest \n",
      "neighbor classifier. The feature map classifier provided best overall performance. It \n",
      "used combined supervised/unsupervised training and attained the same error rate as a \n",
      "k-nearest neighbor classifier, but with fewer supervised training tokens. Furthermore, \n",
      "it required fewer nodes then a k-nearest neighbor classifier. \n",
      "REFERENCES \n",
      "[1] J. \n",
      "[2] D. \n",
      "[3] v. \n",
      "S. Albus, Brains, Behavior, and Robotics. McGraw-Hill, Petersborough, N.H., 1981. \n",
      "J. Burr, \"A neural network digit recognizer,\" in Proceedings of the International Conference \n",
      "on Systems, Man, and Cybernetics, IEEE, 1986. \n",
      "B. Cooper and J. H. Freeman, \"On the asymptotic improvement in the outcome of supervised \n",
      "learning provided by aAditional nonsupervised learning,\" IEEE Transactions on Computers, \n",
      "vol. C-19, pp. 1055-63, November 1970. \n",
      "[4] R. O. Duda and P. E. Hart, Pattern Classification and Scene Analysis. John-Wiley & Sons, New \n",
      "York, 1973. \n",
      "[5] W.Y. Huang and R. P. Lippmann, \"Comparisons between conventional and neural net classifiers,\" \n",
      "in 1st International Conference on Neural Network, IEEE, June 1987. \n",
      "[6] T. \n",
      "Kohonen, K. Makisara, and T. Saramaki, \"Phonotopic maps -- insightful representation of \n",
      "phonological features for speech recognition,\" in Proceedings of the 7th International Confer- \n",
      "ence on Pattern Recognition, IEEE, August 1984. \n",
      "[7] R. P. Lippmann, \"An introduction to computing with neural nets,\" IEEE ASSP Magazine, vol. 4, \n",
      "pp. 4-22, April 1987. \n",
      "[8] R. P. Lippmann and B. Gold, \"Neural classifiers useful for speech recognition,\" in 1st International \n",
      "Conference on Neural Network, IEEE, June 1987. \n",
      "[g] I.D. Longstaff and J. F. Cross, \"A pattern recognition approach to understanding the multi-layer \n",
      "perceptton,\" Mem. 3936, Royal Signals and Radar Establishment, July 1986. \n",
      "[10] \n",
      "[11] \n",
      "[12] r. \n",
      "[131 R. \n",
      "[14] A. \n",
      "N.J. Nilsson, Learning Machines. McGraw Hill, N.Y., 1965. \n",
      "T. Paxsons, Voice and Speech Processing. McGraw-Hill, New York, 1986. \n",
      "Rosenblurt, Percepttons and the Theory of Brain Mechanisms. Spartan Books, 1962. \n",
      "C. Singleton, \"A test for linear separability as applied to self-organizing machines,\" in Self- \n",
      "Organization Systems, 196�, (M. C. Yovits, G. T. Jacobi, and G. D. Goldstein, eds.), pp. 503- \n",
      "524, Spartan Books, Washington, 1962. \n",
      "Wieland and R. Leighton, \"Geometric analysis of neural network capabilities,\" in Ist Interna- \n",
      "tional Conference on Neural Networks, IEEE, June 1987. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
